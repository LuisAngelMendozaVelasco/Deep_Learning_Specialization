# Natural Language Processing & Word Embeddings

Natural language processing with deep learning is a powerful combination. Using word vector representations and embedding layers, train recurrent neural networks with outstanding performance across a wide variety of applications, including sentiment analysis, named entity recognition and neural machine translation.

## Learning Objectives

- Explain how word embeddings capture relationships between words
- Load pre-trained word vectors
- Measure similarity between word vectors using cosine similarity
- Use word embeddings to solve word analogy problems such as Man is to Woman as King is to ______.
- Reduce bias in word embeddings
- Create an embedding layer in Keras with pre-trained word vectors
- Describe how negative sampling learns word vectors more efficiently than other methods
- Explain the advantages and disadvantages of the GloVe algorithm
- Build a sentiment classifier using word embeddings
- Build and train a more sophisticated classifier using an LSTM

## Introduction to Word Embeddings

- [Video - Word Representation](https://www.coursera.org/learn/nlp-sequence-models/lecture/6Oq70/word-representation)

- [Video - Using Word Embeddings](https://www.coursera.org/learn/nlp-sequence-models/lecture/qHMK5/using-word-embeddings)

- [Video - Properties of Word Embeddings](https://www.coursera.org/learn/nlp-sequence-models/lecture/S2mat/properties-of-word-embeddings)

- [Video - Embedding Matrix](https://www.coursera.org/learn/nlp-sequence-models/lecture/K604Z/embedding-matrix)

## Learning Word Embeddings: Word2vec & GloVe

- [Video - Learning Word Embeddings](https://www.coursera.org/learn/nlp-sequence-models/lecture/APM5s/learning-word-embeddings)

- [Video - Word2Vec](https://www.coursera.org/learn/nlp-sequence-models/lecture/8CZiw/word2vec)

- [Video - Negative Sampling](https://www.coursera.org/learn/nlp-sequence-models/lecture/Iwx0e/negative-sampling)

- [Reading - Clarifications about Upcoming GloVe Word Vectors Video](https://www.coursera.org/learn/nlp-sequence-models/supplement/Kz71Z/clarifications-about-upcoming-glove-word-vectors-video)

- [Video - GloVe Word Vectors](https://www.coursera.org/learn/nlp-sequence-models/lecture/IxDTG/glove-word-vectors)

## Applications Using Word Embeddings

- [Video - Sentiment Classification](https://www.coursera.org/learn/nlp-sequence-models/lecture/Jxuhl/sentiment-classification)

- [Video - Debiasing Word Embeddings](https://www.coursera.org/learn/nlp-sequence-models/lecture/zHASj/debiasing-word-embeddings)

## Lecture Notes (Optional)

- [Reading - Lecture Notes W2](./Readings/C5_W2.pdf)

## Programming Assignments

- [Lab - Operations on Word Vectors - Debiasing](./Labs/Operations_on_word_vectors_v2a.ipynb)

- [Lab - Emojify](./Labs/Emoji_v3a.ipynb)