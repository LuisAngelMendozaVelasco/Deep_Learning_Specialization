# Transformer Network

## Learning Objectives

- Create positional encodings to capture sequential relationships in data
- Calculate scaled dot-product self-attention with word embeddings
- Implement masked multi-head attention
- Build and train a Transformer model
- Fine-tune a pre-trained transformer model for Named Entity Recognition
- Fine-tune a pre-trained transformer model for Question Answering
- Implement a QA model in TensorFlow and PyTorch
- Fine-tune a pre-trained transformer model to a custom dataset
- Perform extractive Question Answering

## Transformers

- [Video - Transformer Network Intuition](https://www.coursera.org/learn/nlp-sequence-models/lecture/YKatU/transformer-network-intuition)

- [Video - Self-Attention](https://www.coursera.org/learn/nlp-sequence-models/lecture/lsvRK/self-attention)

- [Video - Multi-Head Attention](https://www.coursera.org/learn/nlp-sequence-models/lecture/jsV2q/multi-head-attention)

- [Video - Transformer Network](https://www.coursera.org/learn/nlp-sequence-models/lecture/Kf5Y3/transformer-network)

## Lecture Notes (Optional)

- [Reading - Lecture Notes W4](./Readings/C5_W4.pdf)

## Programming Assignment

- [Lab - Transformers Architecture with TensorFlow](./Labs/C5_W4_A1_Transformer_Subclass_v1.ipynb)

## Transformer Applications - Ungraded Labs

- [Lab - Transformer Pre-processing](./Labs/Embedding_plus_Positional_encoding.ipynb)

- [Lab - Transformer Network Application: Named-Entity Recognition](./Labs/Transformer_application_Named_Entity_Recognition.ipynb)

- [Lab - Transformer Network Application: Question Answering](./Labs/QA_dataset.ipynb)

- [Reading - Transformers using Trax Library](https://www.coursera.org/learn/nlp-sequence-models/supplement/6DSFF/transformers-using-trax-library)

## Conclusion

- [Video - Conclusion and Thank You!](https://www.coursera.org/learn/nlp-sequence-models/lecture/ATXdW/conclusion-and-thank-you)

## References & Acknowledgments

- [Reading - References](https://www.coursera.org/learn/nlp-sequence-models/supplement/Sul1k/references)

- [Reading - Acknowledgments](https://www.coursera.org/learn/nlp-sequence-models/supplement/ri2Iq/acknowledgments)

- [Reading - (Optional) Opportunity to Mentor Other Learners](https://www.coursera.org/learn/nlp-sequence-models/supplement/P1ZSL/optional-opportunity-to-mentor-other-learners)