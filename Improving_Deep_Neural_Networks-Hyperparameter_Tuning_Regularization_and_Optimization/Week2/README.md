# Optimization Algorithms

Develop your deep learning toolbox by adding more advanced optimizations, random minibatching, and learning rate decay scheduling to speed up your models.

## Learning Objectives

- Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
- Use random minibatches to accelerate convergence and improve optimization
- Describe the benefits of learning rate decay and apply it to your optimization

## Optimization Algorithms

- [Video - Mini-batch Gradient Descent](https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent)

- [Video - Understanding Mini-batch Gradient Descent](https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent)

- [Video - Exponentially Weighted Averages](https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages)

- [Video - Understanding Exponentially Weighted Averages](https://www.coursera.org/learn/deep-neural-network/lecture/Ud7t0/understanding-exponentially-weighted-averages)

- [Video - Bias Correction in Exponentially Weighted Averages](https://www.coursera.org/learn/deep-neural-network/lecture/XjuhD/bias-correction-in-exponentially-weighted-averages)

- [Video - Gradient Descent with Momentum](https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum)

- [Video - RMSprop](https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop)

- [Reading - Clarification about Upcoming Adam Optimization Video](https://www.coursera.org/learn/deep-neural-network/supplement/5fSys/clarification-about-upcoming-adam-optimization-video)

- [Video - Adam Optimization Algorithm](https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm)

- [Reading - Clarification about Learning Rate Decay Video](https://www.coursera.org/learn/deep-neural-network/supplement/4DwoT/clarification-about-learning-rate-decay-video)

- [Video - Learning Rate Decay](https://www.coursera.org/learn/deep-neural-network/lecture/hjgIA/learning-rate-decay)

- [Video - The Problem of Local Optima](https://www.coursera.org/learn/deep-neural-network/lecture/RFANA/the-problem-of-local-optima)

## Lecture Notes (Optional)

- [Reading - Lecture Notes W2](./Readings/C2_W2.pdf)

## Programming Assignment

- [Lab - Optimization Methods](./Labs/Optimization_methods.ipynb)

## Heroes of Deep Learning (Optional)

- [Video - Yuanqing Lin Interview](https://www.coursera.org/learn/deep-neural-network/lecture/CXqid/yuanqing-lin-interview)